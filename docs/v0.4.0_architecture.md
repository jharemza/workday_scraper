# v0.4.0 Architechture

## 1. Overview

**Goal:** Introduce config-driven multi-institution scraping.

**Context:** The original implementation had hardcoded logic for M&T Bank. This milestone modularizes the scraper and introduces YAML-based configuration to support scraping multiple institutions via a reusable interface.

---

## 2. YAML Config Structure (`config/institutions.yaml`)

```yaml
institutions:
  - name: "M&T Bank"
    workday_url: "https://mtb.wd5.myworkdayjobs.com/wday/cxs/mtb/MTB/jobs"
    locations:
      - "Remote, USA"
      - "Buffalo, NY"
    search_text: "sql"
```

- `name`: Display name; used in filenames, logs, and Notion filtering
- `workday_url`: The institution's Workday job endpoint
- `locations`: Filters jobs by `descriptor` values from Workday facets
- `search_text`: Optional keyword filtering for job titles (e.g., `"data"`)

## 3. Architecture Overview

| Module                  | Responsibility                                        |
| ----------------------- | ----------------------------------------------------- |
| `scraper.py`            | Entrypoint: loads config and executes per institution |
| `config_loader.py`      | Parses YAML config and returns institution list       |
| `institution_runner.py` | Handles scraping, deduplication, and uploading jobs   |
| `notion_client.py`      | Manages Notion API interactions and Req ID filtering  |
| `job_parser.py`         | Converts HTML job descriptions into Notion blocks     |

## 4. Notion Integration Rules

- All scraped jobs write to a **single Notion instance** using:

  - `DATABASE_ID`: All jobs to apply
  - `APPLIED_DATABASE_ID`: Already applied jobs

- Only one Notion token (`NOTION_TOKEN`) is used

- Filtering uses `Company == institution["name"]` to:
  - Identify existing jobs
  - Prevent duplicate uploads across institutions

## 5. Logging & Output Conventions

- All logs are written to `scraper.log` (rotating: 5MB max per file, 5 backups)
- Console and file log messages are prefixed with `[Institution Name]`
- Logging covers:

  - Location facet matching
  - Job collection and pagination
  - Req ID filtering (skip detection)
  - Notion upload (success/failure)

- Scraped job data is written to:

```bash
json_output/workday_response_<institution>.json
```

Example:

```bash
json_output/workday_response_MandT_Bank.json
```

## 6. Required Environment Variables (`.env`)

| Variable              | Description                               |
| --------------------- | ----------------------------------------- |
| `NOTION_TOKEN`        | Notion integration token                  |
| `DATABASE_ID`         | ID for the job collection database        |
| `APPLIED_DATABASE_ID` | ID for the applied jobs tracking database |

## 7. Regression Test: M&T Bank

- ✅ M&T Bank tested using the new `InstitutionRunner` abstraction
- ✅ Jobs collected and filtered correctly
- ✅ Salary ranges and job descriptions parsed as expected
- ✅ Notion upload logic remained consistent
- ✅ Output file matched pre-refactor behavior
